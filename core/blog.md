# Development Timeline

## Day 1: Initialization, Embedding Matrices, Forward Pass
**Goal**: Get output predictions from input char
- Set up embedding matrices
- Implemented basic forward pass
- Character-level tokenization

## Day 2: Manual Backpropagation
**Goal**: Calculate and update model weights through stochastic gradient descent
- Built gradient computation from scratch
- Implemented weight updates
- Created training loop

## Day 3: Transformer Block
**Goal**: Build self attention block, wrap in transformer head
- Multi-head self-attention mechanism
- Query, Key, Value matrix operations
- Attention score calculations

## Day 4: Assembling the Model
**Goal**: Combine all the pieces into a working, trainable model
- Integrated all components
- End-to-end training pipeline
- Model serialization

## Day 5: Building the Interactive UI
**Goal**: Create model interface with controls and visualization
- Temperature and input controls
- Real-time text generation
- Attention heatmap visualization

## Day 6: Modern Transformer Architecture
**Goal**: Advanced transformer features
- Rotary Positional Embeddings (RoPE)
- Performance optimizations